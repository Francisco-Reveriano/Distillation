{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb727f7e-38fd-4ef7-b53d-f59277097c5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/ubuntu/.local/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (4.48.3)\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (2.5.1)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (1.3.5)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (1.21.5)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /usr/lib/python3/dist-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers torch pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4662b-78ce-496f-875b-f3780066fd78",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d925ab-9c2e-4268-abe3-3a08a77e9ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>solution</th>\n",
       "      <th>problem</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\boxed{10^-4 eV}</td>\n",
       "      <td>Two quantum states with energies E1 and E2 hav...</td>\n",
       "      <td>Physics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\boxed{11}</td>\n",
       "      <td>trans-cinnamaldehyde was treated with methylma...</td>\n",
       "      <td>Chemistry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           solution                                            problem  \\\n",
       "0  \\boxed{10^-4 eV}  Two quantum states with energies E1 and E2 hav...   \n",
       "1        \\boxed{11}  trans-cinnamaldehyde was treated with methylma...   \n",
       "\n",
       "      domain  \n",
       "0    Physics  \n",
       "1  Chemistry  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/hendrydong/gpqa_diamond/data/test-00000-of-00001.parquet\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efaf555-4bdb-4153-84d3-418eacdfe002",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffea0caf-6836-48fe-b7d1-cd1e1495e269",
   "metadata": {},
   "source": [
    "### Load Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e81a04-d271-4f58-9378-cdcfabc5ef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 19:43:25.198136: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739130205.213342  102622 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739130205.217369  102622 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b6ef10-eba3-4540-a447-c6e4d686d616",
   "metadata": {},
   "source": [
    "### Results with Original  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63738f2e-4125-4723-bd01-b99e1f94af80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Responses: 100%|██████████| 198/198 [01:59<00:00,  1.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "import torch\n",
    "\n",
    "def format_qwen_prompt(system_message: str, user_message: str):\n",
    "    \"\"\"\n",
    "    Formats the input prompt for Qwen2.5 models using ChatML format.\n",
    "\n",
    "    Args:\n",
    "        system_message (str): The system-level instruction.\n",
    "        user_message (str): The user query.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted prompt.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"<|im_start|>system\\n{system_message}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{user_message}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def generate_response(system_message, user_message, device):\n",
    "    formatted_prompt = format_qwen_prompt(system_message, user_message)\n",
    "\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating Responses\"):\n",
    "    system_message = \"You are a helpful AI assistant.\"\n",
    "    df.loc[index, \"Original_Model\"] = generate_response(system_message, df.loc[index, \"problem\"], device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca4bd39-5015-4bb3-a8ed-eade98ca54a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system\\nYou are a helpful AI assistant.\\nuser\\nTwo quantum states with energies E1 and E2 have a lifetime of 10^-9 sec and 10^-8 sec, respectively. We want to clearly distinguish these two energy levels. Which one of the following options could be their energy difference so that they can be clearly resolved?\\n\\nassistant\\nTo determine if two quantum states can be clearly distinguished, we need to consider the energy resolution limit given'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Original_Model\"].head(5)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc91196-c14b-40c8-8814-71641516f458",
   "metadata": {},
   "source": [
    "## Results with New Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e92f4a-4f0c-4946-a388-db9ceec17d81",
   "metadata": {},
   "source": [
    "### Load New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3930f72-ea6d-4206-b4a3-3560d0ca608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? I'm here to help with your writing. What would you like assistance with today?\n",
      "A text is\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Replace 'path/to/your/output_dir' with the actual path (e.g., args.output_dir)\n",
    "model_path = \"../ckpts/s1-20250209_183153\"\n",
    "\n",
    "# Load the model and tokenizer from the directory where you saved them\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "## Tokenize with padding (if needed) and explicitly get the attention mask\n",
    "encoded_inputs = tokenizer(\n",
    "    \"Hello, how are you?\", \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True  # or specify max_length if desired\n",
    ")\n",
    "\n",
    "input_ids = encoded_inputs[\"input_ids\"]\n",
    "attention_mask = encoded_inputs[\"attention_mask\"]\n",
    "\n",
    "# Pass the attention_mask to the model (or generate method)\n",
    "outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a164c12-dac3-4b24-accf-e70410eb1572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Responses: 100%|██████████| 198/198 [01:57<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_from_pretrained(user_input, tokenizer): \n",
    "    # Tokenize with padding (if needed) and explicitly get the attention mask\n",
    "    encoded_inputs = tokenizer(\n",
    "        user_input, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True  # or specify max_length if desired\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    attention_mask = encoded_inputs[\"attention_mask\"]\n",
    "\n",
    "    # Move tensors to GPU by reassigning the returned tensors\n",
    "    input_ids = input_ids.to(\"cuda\")\n",
    "    attention_mask = attention_mask.to(\"cuda\")\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(\"cuda\")\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating Responses\"):\n",
    "    system_message = \"You are a helpful AI assistant.\"\n",
    "    df.loc[index, \"New_Model\"] = generate_from_pretrained(df.loc[index, \"problem\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06eebcd1-880c-41d8-99ca-3db432a297e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"Results.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
