{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb727f7e-38fd-4ef7-b53d-f59277097c5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/ubuntu/.local/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (4.48.3)\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (2.5.1)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (1.3.5)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (1.21.5)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (4.67.1)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.9/250.9 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /usr/lib/python3/dist-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Collecting et-xmlfile\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (1.26.5)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers torch pandas numpy tqdm openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4662b-78ce-496f-875b-f3780066fd78",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d925ab-9c2e-4268-abe3-3a08a77e9ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>solution</th>\n",
       "      <th>problem</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\boxed{10^-4 eV}</td>\n",
       "      <td>Two quantum states with energies E1 and E2 hav...</td>\n",
       "      <td>Physics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\boxed{11}</td>\n",
       "      <td>trans-cinnamaldehyde was treated with methylma...</td>\n",
       "      <td>Chemistry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           solution                                            problem  \\\n",
       "0  \\boxed{10^-4 eV}  Two quantum states with energies E1 and E2 hav...   \n",
       "1        \\boxed{11}  trans-cinnamaldehyde was treated with methylma...   \n",
       "\n",
       "      domain  \n",
       "0    Physics  \n",
       "1  Chemistry  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/hendrydong/gpqa_diamond/data/test-00000-of-00001.parquet\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efaf555-4bdb-4153-84d3-418eacdfe002",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffea0caf-6836-48fe-b7d1-cd1e1495e269",
   "metadata": {},
   "source": [
    "### Load Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e81a04-d271-4f58-9378-cdcfabc5ef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 02:36:51.117570: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739155011.132225   64369 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739155011.136215   64369 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b6ef10-eba3-4540-a447-c6e4d686d616",
   "metadata": {},
   "source": [
    "### Results with Original  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83cfaea9-1aa8-4e87-abc0-356fdc6df70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63738f2e-4125-4723-bd01-b99e1f94af80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Responses: 100%|██████████| 198/198 [02:10<00:00,  1.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "import torch\n",
    "\n",
    "def format_qwen_prompt(system_message: str, user_message: str):\n",
    "    \"\"\"\n",
    "    Formats the input prompt for Qwen2.5 models using ChatML format.\n",
    "\n",
    "    Args:\n",
    "        system_message (str): The system-level instruction.\n",
    "        user_message (str): The user query.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted prompt.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"<|im_start|>system\\n{system_message}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{user_message}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def generate_response(system_message, user_message, device):\n",
    "    formatted_prompt = format_qwen_prompt(system_message, user_message)\n",
    "\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating Responses\"):\n",
    "    system_message = \"You are a helpful AI assistant. Succintly answer each question\"\n",
    "    df.loc[index, \"Original_Model\"] = generate_response(system_message, df.loc[index, \"problem\"], device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca4bd39-5015-4bb3-a8ed-eade98ca54a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system\\nYou are a helpful AI assistant.\\nuser\\nTwo quantum states with energies E1 and E2 have a lifetime of 10^-9 sec and 10^-8 sec, respectively. We want to clearly distinguish these two energy levels. Which one of the following options could be their energy difference so that they can be clearly resolved?\\n\\nassistant\\nTo resolve two quantum states, we use the concept of the resolution limit in spectroscopy, which is'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Original_Model\"].head(5)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc91196-c14b-40c8-8814-71641516f458",
   "metadata": {},
   "source": [
    "## Results with New Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e92f4a-4f0c-4946-a388-db9ceec17d81",
   "metadata": {},
   "source": [
    "### Load New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3930f72-ea6d-4206-b4a3-3560d0ca608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Texas?\n",
      "The capital of Texas is Austin.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* **Austin:**  It's the\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Replace 'path/to/your/output_dir' with the actual path (e.g., args.output_dir)\n",
    "model_path = \"../ckpts/s1_20250213_023116\"\n",
    "\n",
    "# Load the model and tokenizer from the directory where you saved them\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "## Tokenize with padding (if needed) and explicitly get the attention mask\n",
    "encoded_inputs = tokenizer(\n",
    "    \"What is the capital of Texas\", \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True  # or specify max_length if desired\n",
    ")\n",
    "\n",
    "input_ids = encoded_inputs[\"input_ids\"]\n",
    "attention_mask = encoded_inputs[\"attention_mask\"]\n",
    "\n",
    "# Pass the attention_mask to the model (or generate method)\n",
    "outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d35ddb2-1231-46c7-b723-fac0185b4d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<|im_start|>think\n",
       "Here's a concise answer:\n",
       "\n",
       "The capital of Texas is Austin.\n",
       "\n",
       "Here're some reasons whyAustin is the capital:\n",
       "\n",
       "1. Location: Austin is located in the heart of Texas, near the Texas-Louisiana border.\n",
       "\n",
       "2. History: Austin was founded in 1840 as the capital of the Republic of Texas. It's known for being the birthplace of President John F. Kennedy and former U.S. president Bill Clinton.\n",
       "\n",
       "3. Culture: Austin is known for its arts scene, cuisine, and vibrant downtown area.\n",
       "\n",
       "4. Economy: The economy of Texas is primarily centered around agriculture (especially cattle breeding), construction, transportation, and services.\n",
       "\n",
       "5. Population: As of the 2020 census, the city of Austin has a population of approximately 697,000 people.\n",
       "\n",
       "6. Major Cities Nearby: Other major cities in Texas include Houston (the state capital) and Dallas-Fort Worth.\n",
       "\n",
       "7. Transportation: Austin is connected to many other major cities through Interstate highways and railroads.\n",
       "\n",
       "8. Tourism: The city hosts events like the annual Texas State Fair and the annual Grammy Awards ceremony.\n",
       "\n",
       "Austin's nickname is \"The Rock City\" due to its rock music heritage, but it's also famous for its food, culture, and business community.\n",
       "<|im_start|>answer\n",
       "Answer: The capital of Texas is Austin. Here are some reasons why:\n",
       "\n",
       "1. **Location:** Austin is situated in the southeastern part of Texas, near the border with Louisiana.\n",
       "\n",
       "2. **History:** Austin was founded in 1840 by settlers from Kentucky, Missouri, Tennessee, and Illinois. It was originally called Richmond and was renamed after General William Tecumseelus Austin, who fought in the War of 1812.\n",
       "\n",
       "3. **Cultural Impact:** Austin has been a significant cultural center throughout its history. Its thriving arts scene, culinary delights, and diverse population contribute to its status as the second-largest metropolitan area in Texas after Houston.\n",
       "\n",
       "4. **Economic Importance:** The region around Austin is highly economic, with industries including agriculture (particularly livestock farming), construction, transportation, and services.\n",
       "\n",
       "5. **Population:** As of the 2020 United States Census, the city of Austin has a population of approximately 697,000 residents.\n",
       "\n",
       "6. **Notable People:** The mayor of Austin, Kevin Thibodeaux, is known for his political career and has been involved in various public service endeavors.\n",
       "\n",
       "7. **Key Industries:** Some of the most prominent industries in Austin include:\n",
       "\n",
       "   * **Agriculture:** Specializing in beef cattle ranching and poultry production.\n",
       "   * **Construction:** A major employer, especially in the oil and gas industry.\n",
       "   * **Transportation:** Including ports and airports that support the booming tourism industry.\n",
       "   * **Services:** From hotels and restaurants to healthcare facilities and educational institutions.\n",
       "\n",
       "8. **Neighbors:** Austin is adjacent to several other major cities in Texas, such as Houston (the state capital) and Dallas-Fort Worth. These areas provide further opportunities for commerce, entertainment, and education.\n",
       "\n",
       "9. **Nickname:** One of the city's nicknames is \"The Rock City,\" derived from its association with the rock music genre in Texas, which flourished during the early 20th century.\n",
       "\n",
       "10. **U.S. Census Data (2020):** The city of Austin had a population of 697,000 people."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def query_qwen2_5(user_message: str, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Queries the Qwen2.5-14-Instruct model with the provided user_message\n",
    "    and returns the assistant's response.\n",
    "\n",
    "    Args:\n",
    "        user_message (str): The user query.\n",
    "        model: The loaded Qwen2.5 model.\n",
    "        tokenizer: The Qwen2.5 tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        str: The assistant's reply.\n",
    "    \"\"\"\n",
    "    # Define the system message\n",
    "    system_message = \"You are a helpful AI assistant. Succinctly answer the provided question.\"\n",
    "\n",
    "    # Format the prompt using ChatML format\n",
    "    formatted_prompt = (\n",
    "        f\"<|im_start|>system\\n{system_message}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{user_message}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "\n",
    "    # Set the device and prepare inputs\n",
    "    device = \"cuda\"\n",
    "    encoded_inputs = tokenizer(formatted_prompt,\n",
    "                               return_tensors=\"pt\",\n",
    "                               padding=False)\n",
    "    inputs = encoded_inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded_inputs[\"attention_mask\"].to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Generate the model output with a sufficient token budget and proper EOS handling\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=32768,\n",
    "        eos_token_id=tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "    )\n",
    "\n",
    "    # Decode the output (keeping the special tokens for extraction)\n",
    "    raw_output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "    # Extract the assistant's response from the output\n",
    "    assistant_part = raw_output.split(\"<|im_start|>assistant\")[-1]\n",
    "    assistant_response = assistant_part.split(\"<|im_end|>\")[0].strip()\n",
    "\n",
    "    return assistant_response\n",
    "\n",
    "user_message = \"What is the capital of Texas\"\n",
    "display(Markdown(query_qwen2_5(user_message, model, tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "513b129b-a531-4bf2-80da-714b06300862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Answer: The capital of Texas is Austin.\n",
       "Here's why:\n",
       "\n",
       "* **Austin:**  Austin is the state capital of Texas.\n",
       "* **Reason for confusion:** There was once a name for the state capital that was similar (Austin), but this has since been changed. The current official name is Austin.\n",
       "* **Geographical context:** Austin is located in Travis County, which is part of Texas.\n",
       "\n",
       "This simple answer directly addresses the question by identifying the state capital of Texas."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def query_qwen2_5_think(user_message: str, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Queries the Qwen2.5-14-Instruct model with the provided user_message\n",
    "    and returns the assistant's response.\n",
    "\n",
    "    Args:\n",
    "        user_message (str): The user query.\n",
    "        model: The loaded Qwen2.5 model.\n",
    "        tokenizer: The Qwen2.5 tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        str: The assistant's reply.\n",
    "    \"\"\"\n",
    "    # Define the system message\n",
    "    system_message = \"You are a helpful AI assistant. Succinctly answer the provided question.\"\n",
    "    system_message2 = \"Think\"\n",
    "    \n",
    "    # Additional query instructing the assistant to think before answering\n",
    "    additional_instruction = (\"Before answering, please take a moment to think about the question \"\n",
    "                              \"and then provide your succinct final answer.\")\n",
    "    \n",
    "    # Format the prompt using ChatML format with an extra user message\n",
    "    formatted_prompt = (\n",
    "        f\"<|im_start|>system\\n{system_message}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{user_message}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>system\\n{system_message2}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>system\\n{additional_instruction}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "\n",
    "    # Set the device and prepare inputs\n",
    "    device = \"cuda\"\n",
    "    encoded_inputs = tokenizer(formatted_prompt,\n",
    "                               return_tensors=\"pt\",\n",
    "                               padding=False)\n",
    "    inputs = encoded_inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded_inputs[\"attention_mask\"].to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Generate the model output with a sufficient token budget and proper EOS handling\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=32768,\n",
    "        eos_token_id=tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "    )\n",
    "\n",
    "    # Decode the output (keeping the special tokens for extraction)\n",
    "    raw_output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "    # Extract the assistant's response from the output\n",
    "    assistant_part = raw_output.split(\"<|im_start|>assistant\")[-1]\n",
    "    assistant_response = assistant_part.split(\"<|im_end|>\")[0].strip()\n",
    "\n",
    "    return assistant_response\n",
    "\n",
    "# Example usage\n",
    "user_message = \"What is the capital of Texas\"\n",
    "display(Markdown(query_qwen2_5_think(user_message, model, tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d278bd93-ea7a-4980-af10-71ee4819f802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a164c12-dac3-4b24-accf-e70410eb1572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Responses:   1%|          | 1/198 [17:26<57:15:22, 1046.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35750/2536586208.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0meval_original_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_35750/2536586208.py\u001b[0m in \u001b[0;36meval_original_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Generating Responses\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"updated_Model\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_qwen2_5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"problem\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Results_Checkpoint_Model_Short.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35750/3101198898.py\u001b[0m in \u001b[0;36mquery_qwen2_5\u001b[0;34m(user_message, model, tokenizer)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Generate the model output with a sufficient token budget and proper EOS handling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     outputs = model.generate(\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2256\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3242\u001b[0m         \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3243\u001b[0;31m         while self._has_unfinished_sequences(\n\u001b[0m\u001b[1;32m   3244\u001b[0m             \u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3245\u001b[0m         ):\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device, cur_len, max_length)\u001b[0m\n\u001b[1;32m   2451\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mthis_peer_finished_flag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2453\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2454\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2455\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def eval_original_model():\n",
    "    # Import Dataset\n",
    "    df = pd.read_parquet(\"hf://datasets/hendrydong/gpqa_diamond/data/test-00000-of-00001.parquet\")\n",
    "\n",
    "    # Import Model\n",
    "    model_name = \"../ckpts/s1_20250213_023116\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating Responses\"):\n",
    "        df.loc[index, \"updated_Model\"] = query_qwen2_5(df.loc[index, \"problem\"], model, tokenizer)\n",
    "        df.to_excel(\"Results_Checkpoint_Model_Short.xlsx\")\n",
    "\n",
    "    return\n",
    "eval_original_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
