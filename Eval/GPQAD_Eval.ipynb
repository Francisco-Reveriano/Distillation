{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8745ac0-c4ab-48ea-a698-9eac5c184ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/ubuntu/.local/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (4.48.3)\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (2.5.1)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (1.3.5)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (1.21.5)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: openpyxl in /home/ubuntu/.local/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: ipywidgets in /home/ubuntu/.local/lib/python3.10/site-packages (8.1.5)\n",
      "Requirement already satisfied: huggingface_hub in /home/ubuntu/.local/lib/python3.10/site-packages (0.28.1)\n",
      "Collecting openai\n",
      "  Downloading openai-1.62.0-py3-none-any.whl (464 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.8/464.8 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /usr/lib/python3/dist-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: et-xmlfile in /home/ubuntu/.local/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /home/ubuntu/.local/lib/python3.10/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/lib/python3/dist-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /home/ubuntu/.local/lib/python3.10/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/lib/python3/dist-packages (from ipywidgets) (7.31.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface_hub) (4.9.0)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 KB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 KB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting anyio<5,>=3.5.0\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 KB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Collecting pydantic<3,>=1.9.0\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 KB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Collecting exceptiongroup>=1.0.2\n",
      "  Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->openai) (2020.6.20)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.27.2\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (1.26.5)\n",
      "Installing collected packages: typing-extensions, sniffio, jiter, h11, exceptiongroup, annotated-types, pydantic-core, httpcore, anyio, pydantic, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.8.0 exceptiongroup-1.2.2 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.8.2 openai-1.62.0 pydantic-2.10.6 pydantic-core-2.27.2 sniffio-1.3.1 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers torch pandas numpy tqdm openpyxl ipywidgets huggingface_hub openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a596da2e-6687-41a7-aefc-b50e47b55ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from IPython.display import Markdown, display\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31ad0f3-7615-4842-bf2f-2c809918fe06",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06150d6d-b676-4adf-bb3f-bdec783e8acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "df = pd.read_csv(\"Data/gpqa_diamond.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5978cfc4-742e-4ce0-ab22-8568f09005f0",
   "metadata": {},
   "source": [
    "## Load Checkpoint Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3c83a7c-bed9-490b-94a3-85f4ba7c4d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'path/to/your/output_dir' with the actual path (e.g., args.output_dir)\n",
    "model_path = \"../ckpts/s1_20250213_023116\"\n",
    "\n",
    "# Load the model and tokenizer from the directory where you saved them\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c87e27-cdfd-42a4-b5c9-068a0d4b4083",
   "metadata": {},
   "source": [
    "## Evaluate Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd9000f0-ab9e-4226-b2ed-f805b786a361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question(row: pd.Series) -> Tuple[str, List[str]]:\n",
    "        question = row['Question']\n",
    "        correct_answer = row['Correct Answer']\n",
    "        incorrect_answers = [\n",
    "            row['Incorrect Answer 1'],\n",
    "            row['Incorrect Answer 2'],\n",
    "            row['Incorrect Answer 3']\n",
    "        ]\n",
    "        options = [correct_answer] + incorrect_answers\n",
    "        shuffled_options = list(enumerate(options))\n",
    "        random.shuffle(shuffled_options)\n",
    "        \n",
    "        formatted_question = f\"{question}\\n\\nOptions:\\n\"\n",
    "        for i, option in shuffled_options:\n",
    "            formatted_question += f\"- {option}\\n\"\n",
    "        \n",
    "        return formatted_question, [option for _, option in shuffled_options]\n",
    "\n",
    "def get_correct_answer(row: pd.Series) -> str:\n",
    "        return row['Correct Answer']\n",
    "\n",
    "def check_answer(model_answer: str, correct_answer: str) -> bool:\n",
    "        try:\n",
    "            return model_answer.strip().lower() == correct_answer\n",
    "        except (ValueError, IndexError):\n",
    "            return False\n",
    "\n",
    "def query_qwen2_5_think(user_message: str, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Queries the Qwen2.5-14-Instruct model with the provided user_message\n",
    "    and returns the assistant's response.\n",
    "\n",
    "    Args:\n",
    "        user_message (str): The user query.\n",
    "        model: The loaded Qwen2.5 model.\n",
    "        tokenizer: The Qwen2.5 tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        str: The assistant's reply.\n",
    "    \"\"\"\n",
    "    # Define the system message\n",
    "    system_message = \"You are a helpful AI assistant. Succinctly answer the provided question.\"\n",
    "    system_message2 = \"Think\"\n",
    "    \n",
    "    # Additional query instructing the assistant to think before answering\n",
    "    additional_instruction = (\"Reply only with the correct option.\")\n",
    "    \n",
    "    # Format the prompt using ChatML format with an extra user message\n",
    "    formatted_prompt = (\n",
    "        f\"<|im_start|>system\\n{system_message}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{user_message}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>system\\n{system_message2}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>system\\n{additional_instruction}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "\n",
    "    # Set the device and prepare inputs\n",
    "    device = \"cuda\"\n",
    "    encoded_inputs = tokenizer(formatted_prompt,\n",
    "                               return_tensors=\"pt\",\n",
    "                               padding=False)\n",
    "    inputs = encoded_inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded_inputs[\"attention_mask\"].to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Generate the model output with a sufficient token budget and proper EOS handling\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=32768,\n",
    "        eos_token_id=tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "    )\n",
    "\n",
    "    # Decode the output (keeping the special tokens for extraction)\n",
    "    raw_output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "    # Extract the assistant's response from the output\n",
    "    assistant_part = raw_output.split(\"<|im_start|>assistant\")[-1]\n",
    "    assistant_response = assistant_part.split(\"<|im_end|>\")[0].strip()\n",
    "\n",
    "    return assistant_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec0c2d-58f4-41d9-bf68-6438cc4a0b02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Responses:   0%|          | 0/198 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating Responses\"):\n",
    "    if index > 100: \n",
    "        break\n",
    "    question = get_question(row)\n",
    "    model_answer = query_qwen2_5_think(question[0], model, tokenizer)\n",
    "    df.loc[index, \"Adjusted_Model_Answer\"] = model_answer\n",
    "    df.to_excel(\"Results_Checkpoint_Model_Short.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36999887-59f7-4822-b28f-632884a80b4e",
   "metadata": {},
   "source": [
    "## Load Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98fd8dca-076e-47da-91d9-8d1ea307b417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 16:18:15.316197: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739463495.330035  139865 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739463495.333897  139865 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb7668d1-a77e-4123-b59c-f19a1681e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_qwen2_5(user_message: str, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Queries the Qwen2.5-14-Instruct model with the provided user_message\n",
    "    and returns the assistant's response.\n",
    "\n",
    "    Args:\n",
    "        user_message (str): The user query.\n",
    "        model: The loaded Qwen2.5 model.\n",
    "        tokenizer: The Qwen2.5 tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        str: The assistant's reply.\n",
    "    \"\"\"\n",
    "    # Define the system message\n",
    "    system_message = \"You are a helpful AI assistant. Succinctly answer the provided question.\"\n",
    "    system_message2 = \"Think\"\n",
    "    \n",
    "    # Additional query instructing the assistant to think before answering\n",
    "    additional_instruction = (\"Reply only with the correct option.\")\n",
    "    \n",
    "    # Format the prompt using ChatML format with an extra user message\n",
    "    formatted_prompt = (\n",
    "        f\"<|im_start|>system\\n{system_message}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{user_message}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>system\\n{system_message2}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>system\\n{additional_instruction}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "\n",
    "    # Set the device and prepare inputs\n",
    "    device = \"cuda\"\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Generate the model output with a sufficient token budget and proper EOS handling\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=32768,\n",
    "        eos_token_id=tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "    )\n",
    "\n",
    "    # Decode the output (keeping the special tokens for extraction)\n",
    "    raw_output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "    # Extract the assistant's response from the output\n",
    "    assistant_part = raw_output.split(\"<|im_start|>assistant\")[-1]\n",
    "    assistant_response = assistant_part.split(\"<|im_end|>\")[0].strip()\n",
    "\n",
    "    return assistant_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c96e8b1-3a0b-42db-a4b1-fc052dd9c905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Responses:  51%|█████     | 101/198 [01:54<01:49,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating Responses\"):\n",
    "    if index > 100: \n",
    "        break\n",
    "    question = get_question(row)\n",
    "    model_answer = query_qwen2_5(question[0], model, tokenizer)\n",
    "    df.loc[index, \"Original_Model_Answer\"] = model_answer\n",
    "    df.to_excel(\"Results_Original_Model_Short.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d30601-1100-4c61-ab1b-c06fc6a26497",
   "metadata": {},
   "source": [
    "## OpenAI GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc3b7c55-8396-4b67-b5b4-bad18f227b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Hello! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e63d2973-af4e-403c-bf33-8408849ace08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Responses:  51%|█████     | 101/198 [25:11<24:11, 14.97s/it] \n"
     ]
    }
   ],
   "source": [
    "def query_openai(question):\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    completion = client.chat.completions.create(\n",
    "          model=\"gpt-4o\",\n",
    "          temperature=0,\n",
    "          messages=[\n",
    "            {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "          ]\n",
    "        )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating Responses\"):\n",
    "    if index > 100: \n",
    "        break\n",
    "    question = get_question(row)\n",
    "    model_answer = query_openai(question[0])\n",
    "    df.loc[index, \"GPT4o_Model_Answer\"] = model_answer\n",
    "    df.to_excel(\"Results_GPT4o.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
